{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL para la tabla review\n",
    "# Se recorre el archivo \"review.json\" para construir nuevos archivos .parque  \n",
    "#  clasificando por a単o, se identifico que la data va desde 2005 hasta 2022, \n",
    "#  por lo tanto se tendran 17 archivos de la forma ... review2019.csv, review2020.csv ... \n",
    "# nota = En la carpeta donde se ejecuta debe estar el archivo \"review.json\" y \n",
    "#        no deben estar archivos review20XX.csv de corridas anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias a utilizar\n",
    "import pandas as pd \n",
    "from IPython.display import clear_output\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Dise単ado para leer el archivo co el nombre indicado para carga incremental \n",
    "# simplemente colocar el archivo .json con el nombre indicado.\n",
    "nombre = 'review' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completado Etapa 1: review35  100.0%\n",
      "actualizando review_2005.csv\n",
      "actualizando review_2006.csv\n",
      "actualizando review_2007.csv\n",
      "actualizando review_2008.csv\n",
      "actualizando review_2009.csv\n",
      "actualizando review_2010.csv\n",
      "actualizando review_2011.csv\n",
      "actualizando review_2012.csv\n",
      "actualizando review_2013.csv\n",
      "actualizando review_2014.csv\n",
      "actualizando review_2015.csv\n",
      "actualizando review_2016.csv\n",
      "actualizando review_2017.csv\n",
      "actualizando review_2018.csv\n",
      "actualizando review_2019.csv\n",
      "actualizando review_2020.csv\n",
      "actualizando review_2021.csv\n",
      "actualizando review_2022.csv\n"
     ]
    }
   ],
   "source": [
    "# Organizar la informacion Carga de datos\n",
    "# Se recorre el archivo \"review.json\" para construir nuevos .csv  \n",
    "#  clasificando por a単o, se identifico que la data va desde 2005 hasta 2022, \n",
    "#  por lo tanto se tendran 17 archivos de la forma ... review2019.csv, review2020.csv ... \n",
    "# nota = En la carpeta donde se ejecuta debe estar el archivo \"review.json\" y \n",
    "#        no deben estar archivos review20XX.csv de corridas anteriores\n",
    "#Tiempo de ejecucion aprox 15min si se elimina la limpieza final (eliminar ducplicados) <9min\n",
    "\n",
    "import pandas as pd \n",
    "from IPython.display import clear_output\n",
    "import os \n",
    "\n",
    "def cargaIncremental(df,nombre ='review'):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    for year in range(2005,2023):\n",
    "        if os.path.isfile(nombre+\"_\"+str(year)+\".csv\"):\n",
    "            #Carga incremental de datos en .csv\n",
    "            print('actualizando '+nombre+\"_\"+str(year)+\".csv\")\n",
    "            filtered_df = df.loc[df['date'].dt.year == year]\n",
    "            filtered_df.to_csv(nombre+\"_\"+str(year)+\".csv\",mode='a', index=False, header=False)\n",
    "        else:\n",
    "            #Primera corrida\n",
    "            print('creando '+nombre+\"_\"+str(year)+\".csv\")\n",
    "            filtered_df = df.loc[df['date'].dt.year == year]\n",
    "            filtered_df.to_csv(nombre+\"_\"+str(year)+\".csv\",index=False)\n",
    "\n",
    "\n",
    "nombre = 'review' \n",
    "total = 0 \n",
    "cont = 0\n",
    "cont_max = 35  #Se sabe que tama単o +200k  se haran 35 partes (chunk) del .json \n",
    "\n",
    "Tamano = 200000\n",
    "# Se recorre por partes el archivo .json\n",
    "for chunk in pd.read_json(nombre+\".json\",chunksize=Tamano, lines=True):\n",
    "        # Visualizacion de avance\n",
    "        cont = cont + 1\n",
    "        clear_output(wait=True)\n",
    "        print('Completado Etapa 1: '+nombre+str(cont)+ \"  \"+ str(round(cont / cont_max * 100, 2)) + '%')\n",
    "        \n",
    "        #Se carga la parte\n",
    "        cargaIncremental(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Borrando duplicados en caso de que existan\n",
    "cont = 0\n",
    "cont_max = 18 \n",
    "#Limpieza final\n",
    "for year in range(2005,2023):\n",
    "    # Visualizacion de avance\n",
    "    cont = cont + 1\n",
    "    clear_output(wait=True)\n",
    "    print('Completado Etapa 2: '+str(cont)+\"/\"+str(cont_max)+\"  \"+ str(round(cont / cont_max * 100, 2)) + '%')\n",
    "    \n",
    "    if os.path.isfile(nombre+\"_\"+str(year)+\".csv\"):\n",
    "        #Revision de datos en .csv\n",
    "        df = pd.read_csv(nombre+\"_\"+str(year)+\".csv\") \n",
    "        filtered_df = df.drop_duplicates().reset_index(drop=True)\n",
    "        filtered_df.to_csv(nombre+\"_\"+str(year)+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completado Etapa: 18/18  100.0%\n"
     ]
    }
   ],
   "source": [
    "#Manejo de columna Texto\n",
    "#tiempo aprox \n",
    "def ETL_texto(df):\n",
    "    #Por ahora solo se borrara la columna texo\n",
    "    df.drop(columns=[\"text\"],inplace=True)\n",
    "\n",
    "\n",
    "cont = 0\n",
    "cont_max = 18 \n",
    "#Limpieza final\n",
    "for year in range(2005,2023):\n",
    "    # Visualizacion de avance\n",
    "    cont = cont + 1\n",
    "    clear_output(wait=True)\n",
    "    print('Completado Etapa: '+str(cont)+\"/\"+str(cont_max)+\"  \"+ str(round(cont / cont_max * 100, 2)) + '%')\n",
    "    \n",
    "    if os.path.isfile(nombre+\"_\"+str(year)+\".csv\"):\n",
    "        #Revision de datos en .csv\n",
    "        df = pd.read_csv(nombre+\"_\"+str(year)+\".csv\") \n",
    "        df.drop(columns=[\"text\"],inplace=True)\n",
    "        df.to_csv(nombre+\"_\"+str(year)+\".csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#falta manejar la columna user_id para cual se necesita el archivo Iduser.parquet.gzip que \n",
    "# debe contener las columnas: Iduser (int) y user_id (String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#falta manejar la columna business_id para cual se necesita el archivo Idbusiness.parquet.gzip que \n",
    "# debe contener las columnas: Ibusiness (int) y business_id (String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completado: 18/18  100.0%\n",
      "actualizando Idreview.csv con review_2022.csv\n"
     ]
    }
   ],
   "source": [
    "#Codigo para ir leyendo los df \"review_20xx.csv\" e ir creando \n",
    "# la tabla Idreview donde se almacenaran la columnas \n",
    "# indice \"Id_review\" (int64) y review_id(str), para despues devolver\n",
    "# el df \"review_20xx.csv\" sin la columna review_id y con un indice Id_review (int64).\n",
    "# nota1 = En la carpeta donde se ejecuta deben estar los archivos \"review_20xx.csv\" y \n",
    "#       no debe estar el archivo tabla Idreview.csv de corridas anteriores.\n",
    "# nota2 = Esta version no modifica los archivos \"review_20xx.csv\" sino que crea \n",
    "#       nuevas versiones  \"Mod_review_20xx.csv\", esto para verificar cambios\n",
    "# nota2 = Ya se identifico previamente que los review_id son unicos, no hay repetidos.\n",
    "#       para hacer la conversion mas rapido no se revisara pero para agregar nuevos\n",
    "#       se deberia verifica que los nuevos review_id(str) no esten reptidos en la \n",
    "#       tabla ya creada Idreview.csv\n",
    "#Tiempo de ejecucion aprox 8 min.\n",
    "\n",
    "def  creacion_Idreview(nombre_csv,indice=0):\n",
    "    df = pd.read_csv(nombre_csv) \n",
    "    \n",
    "    if os.path.isfile('Idreview.csv'):\n",
    "        #Carga incremental de datos \n",
    "        print('actualizando Idreview.csv con '+nombre_csv)\n",
    "        #Se crea el nuevo \"Id_review\" en secuencia con anterior\n",
    "        lista = list(range(indice+1,indice+df.shape[0]+1))\n",
    "        nuevo_indice = lista[-1]\n",
    "        \n",
    "        #se crea nuevo archivo  Mod_review_20xx.csv original \n",
    "        # (Si se cambia 'Mod_'+nombre_csv por nombre_csv no se crea nuevo archivo )\n",
    "        df.insert(0,\"Id_review\",lista)\n",
    "        Idreview = df[[\"Id_review\",\"review_id\"]]\n",
    "        df.drop(columns=[\"review_id\"],inplace=True)\n",
    "        df.to_parquet(nombre+\"_\"+str(year)+'.parquet.gzip',compression='gzip') \n",
    "        #Se complementa tabla 'Idreview.csv'\n",
    "        Idreview.to_csv('Idreview.csv',mode='a', index=False, header=False)\n",
    "        \n",
    "    else:\n",
    "        #Primera corrida\n",
    "        print('creando Idreview.csv')\n",
    "        df.reset_index(inplace=True)\n",
    "        df = df.rename(columns={\"index\": \"Id_review\"})\n",
    "        Idreview = df[[\"Id_review\",\"review_id\"]]\n",
    "        df.drop(columns=[\"review_id\"],inplace=True)\n",
    "        df.to_parquet(nombre+\"_\"+str(year)+'.parquet.gzip',compression='gzip') \n",
    "        Idreview.to_csv('Idreview.csv',index=False)\n",
    "        nuevo_indice =  Idreview.iloc[-1][0]               \n",
    "    return nuevo_indice\n",
    "\n",
    "\n",
    "nombre = 'review' \n",
    "cont = 0\n",
    "cont_max = 18 \n",
    "indice = 0\n",
    "for year in range(2005,2023):\n",
    "    # Visualizacion de avance\n",
    "    cont = cont + 1\n",
    "    clear_output(wait=True)\n",
    "    print('Completado: '+str(cont)+\"/\"+str(cont_max)+\"  \"+ str(round(cont / cont_max * 100, 2)) + '%')\n",
    "    \n",
    "    #Creacion de tabla 'Idreview.csv' y cambio de \"review_id\"(str)  por \"Id_review\"(int64)\n",
    "    if os.path.isfile(nombre+\"_\"+str(year)+\".csv\"):\n",
    "        indice = creacion_Idreview(nombre+\"_\"+str(year)+\".csv\",indice)\n",
    "        \n",
    "df =pd.read_csv('Idreview.csv')\n",
    "df.to_parquet('Idreview.parquet.gzip',compression='gzip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completado: 18/18  100.0%\n"
     ]
    }
   ],
   "source": [
    "# Se borran archivos innecesarios:\n",
    "from os import remove\n",
    "\n",
    "nombre = 'review' \n",
    "cont = 0\n",
    "cont_max = 18\n",
    "for year in range(2005,2023):\n",
    "    # Visualizacion de avance\n",
    "    cont = cont + 1\n",
    "    clear_output(wait=True)\n",
    "    print('Completado: '+str(cont)+\"/\"+str(cont_max)+\"  \"+ str(round(cont / cont_max * 100, 2)) + '%')\n",
    "    \n",
    "    if os.path.isfile(nombre+\"_\"+str(year)+\".csv\"):\n",
    "        remove(nombre+\"_\"+str(year)+\".csv\")\n",
    "\n",
    "if os.path.isfile('Idreview.csv'):\n",
    "    remove('Idreview.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#al final de la corrida deben quedar los archivos .parquet.gzip para armar la base de datos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78a5f5e9430e63759269e8e709c4002b1ad533978ca32d2fcf985e534411cec9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
