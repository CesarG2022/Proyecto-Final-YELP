{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-CgRE4qI5vbS"
      ],
      "mount_file_id": "1C5INK2kAupPLR-XjfO-1vAra2AJ1I0DT",
      "authorship_tag": "ABX9TyNtyZvokVhWB108/yfLyyHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khorneflakes-dev/Proyecto-Final-YELP/blob/main/data_load/incremental_load_business.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ls0IDKfxxLo"
      },
      "outputs": [],
      "source": [
        "!pip install pymysql"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import sqlalchemy\n",
        "from sqlalchemy.types import Integer, VARCHAR, Float"
      ],
      "metadata": {
        "id": "F1xqodA9x2HK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_json('/content/drive/MyDrive/test/Dataset Yelp/business.json', lines=True)"
      ],
      "metadata": {
        "id": "rX4oTYI_x5AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### definiendo funciones de validacion de datos "
      ],
      "metadata": {
        "id": "-CgRE4qI5vbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_validation(path):\n",
        "  import pathlib \n",
        "  if pathlib.Path(path).suffix == '.json':\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def name_col_val(df):\n",
        "  if df.columns.tolist() == ['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n",
        "       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n",
        "       'attributes', 'categories', 'hours']:\n",
        "    return True\n",
        "  else: return False\n",
        "\n",
        "def n_col_val(df):\n",
        "  if df.shape[1] == 14:\n",
        "    return True\n",
        "  else: return False\n",
        "\n",
        "def duplicates_val(df):\n",
        "  df_id_val = pd.read_csv('/content/drive/MyDrive/validator/id_bus_val.csv')\n",
        "  duplicates = 0\n",
        "  for i in df['business_id']:\n",
        "    if i in df_id_val['old_id'].tolist():\n",
        "      print('file contain duplicates')\n",
        "      return False\n",
        "      break\n",
        "    else: return True"
      ],
      "metadata": {
        "id": "19y8sCizyG9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### definiendo funcion de etl"
      ],
      "metadata": {
        "id": "yPobfPSS7w8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_business(df_business):\n",
        "\n",
        "  # parametros para conectarnos al data warehouse\n",
        "  engine = create_engine('mysql+pymysql://root:projectyelp2022@34.176.218.33/prueba')\n",
        "\n",
        "  df_business = df_business.rename(columns={'business_id':'old_id'})\n",
        "  df_business['city'] = df_business['city'].apply(lambda x: x.lower())\n",
        "  df_business['city'] = df_business['city'].str.replace(',','')\n",
        "  df_business['city'] = df_business['city'].str.replace('.','')\n",
        "  df_business['city'] = df_business['city'].apply(lambda x: x.strip())\n",
        "  df_business['city'] = df_business['city'].apply(lambda x: \" \".join(x.split()))\n",
        "  df_business['city'] = df_business['city'].apply(lambda x: x.title())\n",
        "  df_business['categories'] = df_business['categories'].fillna('0')\n",
        "  cat_list = ['Restaurants', 'Hotels & Travel', 'Food', 'Nightlife', 'Active Life', 'Arts & Entertainment', 'Beauty & Spas']\n",
        "\n",
        "  def validator(value):\n",
        "    if value in cat_list:\n",
        "      return 1000\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  def cat_to_list(value):\n",
        "    value = value.split(', ')\n",
        "    value.sort(key=validator, reverse=True)\n",
        "    return ', '.join(value)\n",
        "  \n",
        "  df_business['categories'] = df_business['categories'].apply(lambda x: cat_to_list(x))\n",
        "\n",
        "  city_dict = {'Belle Chase': 'Belle Chasse', 'Abington Township': 'Abington','Ashland City': 'Ashland','Bellefontaine Neighbors': 'Bellefontaine',\n",
        "               'Bellville': 'Belleville','Belleair Blf': 'Belleair Bluffs','Bethel Township': 'Bethel','Bensalem Pa': 'Bensalem',\n",
        "               'Bensalem Township': 'Bensalem','Boise City': 'Boise','Burlington Township': 'Burlington',\"Carney'S Point\": 'Carneys Point',\n",
        "               'Cedar Brook': 'Cedarbrook','/': '','Conshohoeken':'Conshohocken','Delran Township': 'Delran','Delran Twp': 'Delran',\n",
        "               'Concord Township': 'Concord','Deptford Township': 'Deptford','Eastampton Township': 'Eastampton',' Township': '',\n",
        "               'Fairview Hts': 'Fairview','-': '',' City': '',' Twp': '','Bch': 'Beach','Land O Lakes':\"Land O'Lakes\",\"Land O' Lakes\":\"Land O'Lakes\",\n",
        "               'Mccordsville': 'Mc Cordsville','Metarie': 'Metairie','Mt Laurel Twp Nj': 'Mt Laurel','Sqaure': 'Square','O Fallon': \"O'Fallon\",\n",
        "               \"O' Fallon\": \"O'Fallon\",'Phila': 'Philadelphia','Philadephia': 'Philadelphia','Philly': 'Philadelphia',\n",
        "               'Redingtn Shor': 'Redington Shore','Redington Shores': 'Redington Shore','Riverview Fl': 'Riverview','Saintt': 'Saint',\n",
        "               'Tierre': 'Tierra',\"Town 'N' Country\": 'Town & Country','Town And Country': 'Town & Country','Town N Country': 'Town & Country',\n",
        "               'Tuscon': 'Tucson'\n",
        "             }\n",
        "\n",
        "  df_business = df_business.replace({'city': city_dict})\n",
        "  df_restaurants = df_business[df_business['categories'].str.contains('Restaurant')]\n",
        "  \n",
        "  \n",
        "  def cat_to_col(value):\n",
        "    aux_dict = {}\n",
        "    x = value.split(', ')\n",
        "    for i in x:\n",
        "      aux_dict[i] = 1\n",
        "    return aux_dict\n",
        "\n",
        "  def dict_to_columns(df_i, column):\n",
        "    df_i[column] = df_i[column].fillna('{}')\n",
        "    df_i.reset_index(inplace=True)\n",
        "    df_o = df_i.join(pd.json_normalize(df_i.pop(column)))\n",
        "    if 'level_0' in df_o.columns:\n",
        "      df_o.drop(['level_0'], axis=1, inplace=True)\n",
        "\n",
        "    return df_o\n",
        "    \n",
        "  def attr_to_list(i):\n",
        "    attributes_list = []\n",
        "    try:\n",
        "      for j in i:\n",
        "        if i[j] == 'True' or i[j] == '1' or i[j] == \"u'free'\":\n",
        "          attributes_list.append(j)\n",
        "      return ', '.join(attributes_list)\n",
        "    except:\n",
        "      return ''\n",
        "  \n",
        " \n",
        "\n",
        "  \n",
        "  df_restaurants['attributes'] = df_restaurants['attributes'].apply(lambda x: attr_to_list(x))\n",
        "  df_business['attributes'] = df_business['attributes'].apply(lambda x: attr_to_list(x))\n",
        "  df_business = df_business[df_business['categories'].str.contains('Restaurants|Hotels & Travel|Food|Nightlife|Active Life|Arts & Entertainment|Beauty & Spas')]\n",
        "  df_business = df_business.reset_index(drop=True)\n",
        "\n",
        "  business_index = pd.DataFrame()\n",
        "  business_index['id'] = list(range(1, len(df_business) + 1, 1))\n",
        "  business_index['business_id'] = df_business['old_id'].copy()\n",
        "\n",
        "  df_business.drop(['old_id'], axis=1, inplace=True)\n",
        "  new_col = list(range(1, len(df_business) +1, 1))\n",
        "  df_business.insert(loc = 0, column = 'business_id', value = new_col)\n",
        "\n",
        " # creando el dataset de city_state\n",
        "  \n",
        "  last_city_id = engine.connect().execute('select max(city_state_id) from business_city_state;').fetchall()[0][0]\n",
        "\n",
        "  df_city_state = pd.DataFrame()\n",
        "  df_city_state['city'] = df_business['city']\n",
        "  df_city_state['state'] = df_business['state']\n",
        "  df_city_state['aux'] = df_city_state['city'] +', '+  df_city_state['state']\n",
        "  \n",
        "  old_city_state = engine.connect().execute('select concat(city,\", \", state) from business_city_state;').fetchall()\n",
        "\n",
        "  new_city_state = []\n",
        "\n",
        "  city_state_list = df_city_state['aux'].tolist()\n",
        "\n",
        "  for i in city_state_list:\n",
        "    try:\n",
        "      old_city_state.index(i)\n",
        "    except:\n",
        "      new_city_state.append(i)\n",
        "  \n",
        "  df_city_state2 = pd.DataFrame()\n",
        "  df_city_state2['aux'] = new_city_state\n",
        "  df_city_state2.insert(loc = 0, column = 'city_state_id', value = list(range(last_city_id+1, len(df_city_state2)+1+last_city_id))) \n",
        "  df_city_state2['city'] = df_city_state2['aux'].apply(lambda x: x.split(', ')[0])\n",
        "  df_city_state2['state'] = df_city_state2['aux'].apply(lambda x: x.split(', ')[-1])\n",
        "  df_city_state2.drop(['aux'], axis=1, inplace=True)\n",
        "  df_city_state2 = df_city_state2[['city_state_id', 'city', 'state']]\n",
        "  df2 = pd.merge(df_business, df_city_state2, left_on=['city', 'state'], right_on=['city', 'state'], how='inner')\n",
        "\n",
        "  df_business['city_state_id'] = df2['city_state_id']\n",
        "  df_business.drop(['city'], axis=1, inplace=True)\n",
        "  df_business.drop(['state'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "  # creando el dataframe de horas\n",
        "  df_hours = pd.DataFrame()\n",
        "  df_hours['hours'] = df_business['hours'].copy()\n",
        "  df_hours['aux'] = df_hours['hours'].astype('str')\n",
        "  df_hours = dict_to_columns(df_hours, 'hours')\n",
        "  df_hours = df_hours.fillna(0)\n",
        "  df_hours.drop(['index'], axis=1, inplace=True)\n",
        "  df_hours = df_hours.drop_duplicates()\n",
        "  \n",
        "  last_hour_id = engine.connect().execute('select max(hours_id) from business_hours;').fetchall()[0][0]\n",
        "\n",
        "  df_hours.insert(loc = 0, column = 'hours_id', value = list(range(last_hour_id+1, len(df_hours)+1+last_hour_id)))\n",
        "\n",
        "  df_business['hours'] = df_business['hours'].astype('str')\n",
        "  df3 = pd.merge(df_business, df_hours, left_on='hours', right_on='aux', how='inner')\n",
        "  df3.drop(['hours','aux','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'], axis=1, inplace=True)\n",
        "  df_business['hours_id'] = df3['hours_id']\n",
        "  df_business.drop(['hours'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "  # creando el dataframe de attributes\n",
        "\n",
        "  df_business['attributes'] = df_business['attributes'].astype('str')\n",
        "\n",
        "  df_attributes = pd.DataFrame()\n",
        "  \n",
        "  attr_val = (pd.read_sql_table('business_attributes', engine.connect()))['attributes'].tolist()\n",
        "  old_attributes = engine.connect().execute('select categories from business_categories;').fetchall()\n",
        "\n",
        "  new_attr = []\n",
        "  attributes_list = df_business['attributes'].tolist()\n",
        "\n",
        "  for i in attributes_list:\n",
        "    try:\n",
        "      old_attributes.index(i)\n",
        "    except:\n",
        "      new_attr.append(i)\n",
        "  \n",
        "  df_attributes['attributes'] = new_attr\n",
        "\n",
        "  last_attr_id = engine.connect().execute('select max(attributes_id) from business_attributes;').fetchall()[0][0]\n",
        "\n",
        "  df_attributes.insert(loc = 0, column = 'attributes_id', value = list(range(last_attr_id+1, len(df_attributes)+1+last_attr_id)))\n",
        "  df5 = pd.merge(df_business, df_attributes, left_on=['attributes'], right_on=['attributes'], how='inner')\n",
        "  df_business['attributes_id'] = df5['attributes_id']\n",
        "  df_business.drop(['attributes'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "  # creando el dataframe de categories\n",
        "\n",
        "  df_categories = pd.DataFrame()\n",
        "\n",
        "  last_cat_id = engine.connect().execute('select max(categories_id) from business_categories;').fetchall()[0][0]\n",
        "  old_cat = engine.connect().execute('select categories from business_categories;').fetchall()\n",
        "  \n",
        "  new_cat = []\n",
        "  categories_list = df_business['categories'].tolist()\n",
        "  for i in categories_list:\n",
        "    try:\n",
        "      old_cat.index(i)\n",
        "    except:\n",
        "      new_cat.append(i)\n",
        "\n",
        "  df_categories['categories'] = new_cat\n",
        "  df_categories.insert(loc = 0, column = 'categories_id', value = list(range(last_cat_id+1, len(df_categories)+1+last_cat_id)))\n",
        "\n",
        "  df6 = pd.merge(df_business, df_categories, left_on='categories', right_on='categories', how='inner')\n",
        "  df_business['categories_id'] = df6['categories_id']\n",
        "  \n",
        "  df_hours.drop(['aux'], axis=1, inplace=True)\n",
        "  hours_aux = df_hours.columns.tolist()[1:]\n",
        "  df_business.drop('categories', axis=1, inplace=True)\n",
        "  for i in hours_aux:\n",
        "    df_hours[i] = df_hours[i].astype('str')\n",
        "  old_business_id = engine.connect().execute('select max(business_id) from business;').fetchall()[0][0]\n",
        "  df_business['business_id'] = list(range(old_business_id+1, len(df_business)+1+old_business_id))\n",
        "  \n",
        "  df_business.to_sql('business', con=engine, index=False, if_exists='append')\n",
        "  df_hours.to_sql('business_hours', con=engine, index=False, if_exists='append')\n",
        "  df_attributes.to_sql('business_attributes', con=engine, index=False, if_exists='append')\n",
        "  df_categories.to_sql('business_categories', con=engine, index=False, if_exists='append')\n",
        "  df_city_state2.to_sql('business_city_state', con=engine, index=False, if_exists='append')\n",
        "\n",
        "  engine.dispose()"
      ],
      "metadata": {
        "id": "Pd3-iTXJ70jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### demo"
      ],
      "metadata": {
        "id": "Mw6R6xgImLkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_increment = df.iloc[1001:2000]"
      ],
      "metadata": {
        "id": "pHkajkbQrccy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine = create_engine('mysql+pymysql://root:projectyelp2022@34.176.218.33/prueba')\n",
        "sql_demo = engine.connect().execute('select categories from business_categories;').fetchall()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xp8vCCfsC6R",
        "outputId": "c3de831e-43c0-4707-cbd4-1031279882f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_business(df_increment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDoT-6xdCvJD",
        "outputId": "f9b39284-6602-4f1a-a199-f5a8e4abeb2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ejecutando la validacion y etl"
      ],
      "metadata": {
        "id": "WwJMZ67K53PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/test/Dataset Yelp/business.json'\n",
        "\n",
        "if format_validation(path) == True:\n",
        "  df_business = pd.read_json(path, lines=True, nrows=100)\n",
        "  col_val = name_col_val(df_business)\n",
        "  number_cols = n_col_val(df_business)\n",
        "  duplicates = duplicates_val(df_business)\n",
        "  if col_val == True and number_cols == True and duplicates == True:\n",
        "    print('file validated')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDHXOqY6zicf",
        "outputId": "6e779a39-ec13-49e2-99c9-608c39ee9bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file contain duplicates\n"
          ]
        }
      ]
    }
  ]
}