{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de archivos para carga incremental tablas users,bussines, review\n",
    "\n",
    "Preparacion para prueba user y bussines, se utilizara el archivo 'XXXXXX.json' para sacar tres conjuntos de dataset y se crearan 2 mas:<br>\n",
    "    1. 'XXXXXX_inicial.json', se selecciona un 70% del dataset de forma aleatoria    <br>\n",
    "    2. 'XXXXXX_incremental.json', se selecciona un 20% del dataset de forma aleatoria <br>\n",
    "    3. 'XXXXXX_incremental_con_repeticiones.json' se selecciona 10% del dataset \n",
    "        pero se agregan valores de XXXXXX_incremental.json y XXXXXX_inicial.json <br>\n",
    "    4. 'XXXXXX_incremental_con_fallas_columnas.json', Archivo .json con columnas aleatorio<br>\n",
    "    5. 'XXXXXX_incremental_con fallas_formato_datos.json', Archivo .json con columnas que coinciden \n",
    "        con 'XXXXXX_inicial.json' pero su contenido no.\n",
    "\n",
    "Preparacion para prueba user y bussines:<br>\n",
    "    1. Se utilizan los id de las tablas user y bussines para crear la tabla 'review_inicial.json'\n",
    "    2. De los valores que no quedaron en 'review_inicial.json' se crea con la mitad de ellos 'review_incremental.json'\n",
    "    3. De los valores restantes se construye review_incremental_con_repeticiones.json' agregando valores\n",
    "    que se encuentren tambien en 'review_incremental.json' y 'review_inicial.json'\n",
    "    4. 'review_incremental_con_fallas_columnas.json', Archivo .json con columnas aleatorio<br>\n",
    "    5. 'review_incremental_con fallas_formato_datos.json', Archivo .json con columnas que coinciden \n",
    "        con 'review_inicial.json' pero su contenido no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condiciones de incio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias a utilizar\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Direccion incial donde estan archivos de entrada y donde se almacenaran archivos salida\n",
    "path_incial = 'C:\\\\Github\\\\DataSet_YELP\\\\'\n",
    "path_final = 'C:\\\\Github\\\\DataSet_YELP\\\\pruebas_incremental\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion principal para creacion de datos de prueba user y bussines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creacion_archivos_prueba(df,nombre,path_final):\n",
    "    #Arbitrariamente se define:  \n",
    "    #   70% de data en 'XXXXX_inicial.json' , \n",
    "    #   20% en 'XXXXX_incremental.json' y \n",
    "    #   10% en 'XXXXX_incremental_con_repeticiones.json'\n",
    "\n",
    "    corte_0 = int(round(df.shape[0]*0.7,0))\n",
    "    XXXXX_inicial = df.iloc[:corte_0,:]\n",
    "    XXXXX_inicial.to_json(path_final+nombre+'_inicial.json')\n",
    "    print('Creado :'+path_final+nombre+'_inicial.json')\n",
    "\n",
    "    corte_1 = int(round(df.shape[0]*0.9,0))\n",
    "    XXXXX_incremental = df.iloc[corte_0+1:corte_1,:]\n",
    "    XXXXX_incremental.to_json(path_final+nombre+'_incremental.json')\n",
    "    print('Creado :'+path_final+nombre+'_incremental.json')\n",
    "\n",
    "    df1 = df.iloc[corte_1+1:,:]\n",
    "    df2 = XXXXX_incremental.sample(df1.shape[0]//4)\n",
    "    df3 = XXXXX_inicial.sample(df1.shape[0]//4)\n",
    "    XXXXX_repeticiones = pd.concat([df1, df2,df3], ignore_index=True)\n",
    "    XXXXX_repeticiones.to_json(path_final+nombre+'_incremental_con_repeticiones.json')\n",
    "    print('Creado :'+path_final+nombre+'_incremental_con_repeticiones.json')\n",
    "\n",
    "    df = pd.DataFrame(np.random.randn(1000, 4), columns=list('ABCD'))\n",
    "    df.to_json(path_final+nombre+'_aleatorio1.json')\n",
    "    print('Creado :'+path_final+nombre+'_aleatorio1.json')\n",
    "    \n",
    "    columnas = list(df.columns)\n",
    "    df = pd.DataFrame(np.random.randn(1000, len(columnas)), columns=columnas)\n",
    "    df.to_json(path_final+nombre+'_aleatorio2.json')\n",
    "    print('Creado :'+path_final+nombre+'_aleatorio2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion de datos de prueba user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\user_inicial.json\n",
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\user_incremental.json\n",
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\user_incremental_con_repeticiones.json\n",
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\user_aleatorio1.json\n",
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\user_aleatorio2.json\n"
     ]
    }
   ],
   "source": [
    "nombre_archivo = 'user.json'\n",
    "nombre = nombre_archivo.split('.')[0]\n",
    "\n",
    "cont = 0\n",
    "Tamano = 200000\n",
    "for chunk in pd.read_json(path_incial+nombre_archivo,chunksize=Tamano, lines=True):\n",
    "    cont = cont + 1\n",
    "    #Se carga cada parte\n",
    "    if cont == 1:  \n",
    "        user = chunk\n",
    "    else:\n",
    "        user = pd.concat([user, chunk], ignore_index=True)\n",
    "\n",
    "creacion_archivos_prueba(user,nombre,path_final)\n",
    "#tiempo aprox = 4 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion de datos de prueba bussines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\business_inicial.json\n",
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\business_incremental.json\n",
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\business_incremental_con_repeticiones.json\n",
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\business_aleatorio1.json\n",
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\business_aleatorio2.json\n"
     ]
    }
   ],
   "source": [
    "nombre_archivo = 'business.json'\n",
    "nombre = nombre_archivo.split('.')[0]\n",
    "business = pd.read_json(path_incial+nombre_archivo,lines=True)\n",
    "creacion_archivos_prueba(business,nombre,path_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion de datos de prueba review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_archivo = 'review.json'\n",
    "nombre = nombre_archivo.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_archivo = 'review.json'\n",
    "nombre = nombre_archivo.split('.')[0]\n",
    "cont = 0\n",
    "Tamano = 200000\n",
    "for chunk in pd.read_json(path_incial+nombre_archivo,chunksize=Tamano, lines=True):\n",
    "    cont = cont + 1\n",
    "    #Se carga cada parte\n",
    "    if cont == 1:\n",
    "        review = chunk\n",
    "    else:\n",
    "        review = pd.concat([review, chunk], ignore_index=True)\n",
    "#tiempo aprox = 10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inicial = pd.read_json(path_final+'user_inicial.json')\n",
    "ids_user_inicial = user_inicial['user_id']\n",
    "#tiempo aprox 3min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_inicial = pd.read_json(path_final+'business_inicial.json')\n",
    "ids_business_inicial = business_inicial['business_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(ids_business_inicial,review,left_on='business_id',right_on='business_id')\n",
    "review_inicial = pd.merge(ids_user_inicial,df,left_on='user_id',right_on='user_id')\n",
    "#Tiempo aprox = 25min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\review_inicial.json\n"
     ]
    }
   ],
   "source": [
    "review_inicial.to_json(path_final+nombre+'_inicial.json')\n",
    "print('Creado :'+path_final+nombre+'_inicial.json')\n",
    "#tiempo 7min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\review_incremental_completo.json\n"
     ]
    }
   ],
   "source": [
    "listado1 =list(review_inicial['review_id'])\n",
    "review_incremental_completo = review[~review['review_id'].isin(listado1)]\n",
    "review_incremental_completo.to_json(path_final+nombre+'_incremental_completo.json')\n",
    "print('Creado :'+path_final+nombre+'_incremental_completo.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\review_incremental_con_repeticiones.json\n"
     ]
    }
   ],
   "source": [
    "#Arbitrariamente se define:  \n",
    "#   70% en 'review_incremental.json' y \n",
    "#   30% en 'review_incremental_con_repeticiones.json'\n",
    "\n",
    "review_incremental_completo = review_incremental_completo.reset_index(drop=True)\n",
    "corte_0 = int(round(review_incremental_completo.shape[0]*0.7,0))\n",
    "review_incremental = review_incremental_completo.iloc[0:corte_0,:]\n",
    "review_incremental.to_json(path_final+nombre+'_incremental.json')\n",
    "print('Creado :'+path_final+nombre+'_incremental.json')\n",
    "\n",
    "df1 = review_incremental_completo.iloc[corte_0+1:,:]\n",
    "df2 = review_incremental.sample(df1.shape[0]//4)\n",
    "df3 = review_inicial.sample(df1.shape[0]//4)\n",
    "user_incremental_con_repeticiones = pd.concat([df1, df2,df3], ignore_index=True)\n",
    "user_incremental_con_repeticiones.to_json(path_final+nombre+'_incremental_con_repeticiones.json')\n",
    "print('Creado :'+path_final+nombre+'_incremental_con_repeticiones.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\review_aleatorio1.json\n",
      "Creado :C:\\Github\\DataSet_YELP\\pruebas_incremental\\review_aleatorio2.json\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randn(1000, 4), columns=list('ABCD'))\n",
    "df.to_json(path_final+nombre+'_aleatorio1.json')\n",
    "print('Creado :'+path_final+nombre+'_aleatorio1.json')\n",
    "    \n",
    "columnas = list(review_inicial.columns)\n",
    "df = pd.DataFrame(np.random.randn(1000, len(columnas)), columns=columnas)\n",
    "df.to_json(path_final+nombre+'_aleatorio2.json')\n",
    "print('Creado :'+path_final+nombre+'_aleatorio2.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78a5f5e9430e63759269e8e709c4002b1ad533978ca32d2fcf985e534411cec9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
