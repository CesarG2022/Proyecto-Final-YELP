{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sskDioat_Bcz",
        "CaMyvSIRyZgb",
        "IoPLy5zxzijM"
      ],
      "mount_file_id": "1gN6GNT94fkI507Ctjuq2S4yW-pntHC-a",
      "authorship_tag": "ABX9TyNGgU6ytH+Q4EYkl880Embq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khorneflakes-dev/Proyecto-Final-YELP/blob/main/ETL_users.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#descomprime el zip con los datasets de yelp\n",
        "#from zipfile import ZipFile \n",
        "#with ZipFile('drive/MyDrive/Copia de Dataset Yelp.zip', 'r') as zipObj:\n",
        "#  zipObj.extractall('drive/MyDrive/yelp')"
      ],
      "metadata": {
        "id": "PmkLjnTnqvOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PysHorC99Gqo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_columns = None # para mostrar todas las columnas de los dataframes\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## obtaining integer user codes csv. Â¡suprimible!"
      ],
      "metadata": {
        "id": "ErdiJrpZw5-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users = pd.read_json(\"drive/MyDrive/yelp/Dataset Yelp/user.json\", lines=True , chunksize=200000) #lee user.json\n",
        "cods_users = pd.concat(users).user_id\n",
        "cods_users = cods_users.reset_index()\n",
        "cods_users.info()"
      ],
      "metadata": {
        "id": "-kGxyR12bvxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84d622c-ec7d-45a1-a671-0929ba956e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1987897 entries, 0 to 1987896\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Dtype \n",
            "---  ------   ----- \n",
            " 0   index    int64 \n",
            " 1   user_id  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 30.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cods_users.user_id.duplicated().sum()"
      ],
      "metadata": {
        "id": "HFruo6ErdsW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805858f5-636a-49ed-f02a-ff66f8eda691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cods_users = cods_users.rename(columns={'index':'n_user_id'})"
      ],
      "metadata": {
        "id": "XazRfCI2hhB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cods_users.n_user_id.duplicated().sum()"
      ],
      "metadata": {
        "id": "OfalYPF7oYT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee912ef-3459-4990-f0bf-7cc43e587d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cods_users.to_csv('drive/MyDrive/yelp/codes_users.csv', index=False)"
      ],
      "metadata": {
        "id": "I_LoH_kQikXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del cods_users"
      ],
      "metadata": {
        "id": "xHpfEoTTlJSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## user.json ETL"
      ],
      "metadata": {
        "id": "USOhYFalgPjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users = pd.read_json(\"drive/MyDrive/yelp/Dataset Yelp/user.json\", lines=True , chunksize=400000) # definition of the iterator reader object"
      ],
      "metadata": {
        "id": "L9tRDE4ZQhH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_to_drop=['name','elite','compliment_hot','compliment_more', 'compliment_profile', 'compliment_cute', 'compliment_list', 'compliment_note', 'compliment_plain',\n",
        "'compliment_cool', 'compliment_funny', 'compliment_writer','compliment_photos'] # droppable columns list\n",
        "data_types={'average_stars':'uint8' , 'fans':'uint16', 'review_count':'uint16', 'cool':'uint32', 'useful':'uint32', 'funny':'uint32'} # data types dict for some columns\n",
        "\n",
        "chunks=[] # define empty list for chunks\n",
        "for chunk in users:  #iterate over the reader object users\n",
        "  iter_chunk = chunk.drop(columns=col_to_drop).astype(data_types) # drop columns listed in col_to_drop from every chunk and change the data types according to data_types dict\n",
        "  chunks.append(iter_chunk)  # append modified chunks into the chunks list\n",
        "  \n",
        "  \n"
      ],
      "metadata": {
        "id": "af-VEvSVkVe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users = pd.concat(chunks) # concatenate all the chunks in the chunks list to create users dataframe"
      ],
      "metadata": {
        "id": "39sbPWSeGl6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.yelping_since = pd.to_datetime(users.yelping_since, format='%Y-%m-%d %H:%M:%S') # change column 'yelping_since' to datetime"
      ],
      "metadata": {
        "id": "PM5GhTmdMiwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# intento de ahorro de memoria \n",
        "del chunks\n",
        "del chunk\n",
        "del iter_chunk"
      ],
      "metadata": {
        "id": "_aYtUXK-MDoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users = users.reset_index() # create a new column with integers from the index values, this new columns is named automatically as 'index'"
      ],
      "metadata": {
        "id": "rcaD7W2VI8nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users = users.rename(columns={'index':'n_user_id'}) # rename the recently created column of integers as n_user_id\n",
        "users = users.astype({'n_user_id':'uint32'}) # change the type of column n_user_id to uint32"
      ],
      "metadata": {
        "id": "KV6dBbZtMk8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exportar tabla user transformada en formato parquet !!!!Suprimible!!!!\n",
        "# users.drop(columns='user_id').to_parquet('drive/MyDrive/yelp/users.gzip', compression ='gzip',  index=False)"
      ],
      "metadata": {
        "id": "WjO_ngFhLw7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## review.json chunk load to dataframe reviews"
      ],
      "metadata": {
        "id": "sskDioat_Bcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = pd.read_json(\"drive/MyDrive/yelp/Dataset Yelp/review.json\", lines=True , chunksize=200000)"
      ],
      "metadata": {
        "id": "6l2pJeUf4kAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_types={'stars':'uint8' , 'useful':'int16', 'funny':'int16', 'cool':'int16'}\n",
        "chunks=[] # define empty list for chunks\n",
        "for chunk in reviews:  #iterate over the reader object reviews\n",
        "  chunks.append(chunk.astype(data_types)) # change the data types from every chunk according to data_types dict an append to chunks list\n"
      ],
      "metadata": {
        "id": "on1EprSA49JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = pd.concat(chunks, ignore_index=True)"
      ],
      "metadata": {
        "id": "O6DLZcSF5Vj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = reviews.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "jE4TOvBTFky2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.info()"
      ],
      "metadata": {
        "id": "HdC0EVL35YKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d9396c-e3ea-4371-d2b1-972362215745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6990280 entries, 0 to 6990279\n",
            "Data columns (total 9 columns):\n",
            " #   Column       Dtype         \n",
            "---  ------       -----         \n",
            " 0   review_id    object        \n",
            " 1   user_id      object        \n",
            " 2   business_id  object        \n",
            " 3   stars        uint8         \n",
            " 4   useful       int16         \n",
            " 5   funny        int16         \n",
            " 6   cool         int16         \n",
            " 7   text         object        \n",
            " 8   date         datetime64[ns]\n",
            "dtypes: datetime64[ns](1), int16(3), object(4), uint8(1)\n",
            "memory usage: 313.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users.info()"
      ],
      "metadata": {
        "id": "cMFuwF2aAuMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b623cd-a520-4b5b-aa10-f6d59cf4f739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1987897 entries, 0 to 1987896\n",
            "Data columns (total 10 columns):\n",
            " #   Column         Dtype         \n",
            "---  ------         -----         \n",
            " 0   n_user_id      uint32        \n",
            " 1   user_id        object        \n",
            " 2   review_count   uint16        \n",
            " 3   yelping_since  datetime64[ns]\n",
            " 4   useful         uint32        \n",
            " 5   funny          uint32        \n",
            " 6   cool           uint32        \n",
            " 7   friends        object        \n",
            " 8   fans           uint16        \n",
            " 9   average_stars  uint8         \n",
            "dtypes: datetime64[ns](1), object(2), uint16(2), uint32(4), uint8(1)\n",
            "memory usage: 85.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del chunks\n",
        "del chunk"
      ],
      "metadata": {
        "id": "2671Z_MR5-HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exportar tabla user transformada en formato parquet\n",
        "#reviews.drop(columns='user_id').to_parquet('drive/MyDrive/yelp/reviews.gzip', compression ='gzip',  index=False)"
      ],
      "metadata": {
        "id": "VD-T4ulh7eV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## change of alphanumerics ids for integer ids : it's found 33 reviews without user_id"
      ],
      "metadata": {
        "id": "CaMyvSIRyZgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.user_id.unique().shape"
      ],
      "metadata": {
        "id": "zWRfie995PGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139459ea-eb71-43e9-dfd9-021ad78f96cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1987929,)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = reviews.merge(users[['user_id','n_user_id']] , how='inner' , on='user_id') # inserta la columna n_user_id en la tabla reviews\n"
      ],
      "metadata": {
        "id": "7DNPsoj8zOhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(reviews[reviews.n_user_id.isnull()]) # busca reviews de ususarios que no estan en la tabla de users"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqS6mxjX2Dk4",
        "outputId": "2d6019a8-4425-4144-d7a1-01f7beeea020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "VkXVIWte7rqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear una base de datos con sqlite3"
      ],
      "metadata": {
        "id": "IoPLy5zxzijM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATING THE TABLE\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('drive/MyDrive/yelp/PFDB.db')\n",
        "print(\"Opened database successfully\");\n",
        "\n",
        "\n",
        "reviews.to_sql('reviews', conn, if_exists='replace')\n",
        "\n",
        "conn.execute(\n",
        "    \"\"\"\n",
        "    create table reviews as \n",
        "    select * from reviews\n",
        "    \"\"\")\n",
        "\n",
        "print(\"Table created successfully\");\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7Kz8xgqzqED",
        "outputId": "bca94107-0a7d-4abf-df2d-147be199804c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opened database successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pyspark"
      ],
      "metadata": {
        "id": "y286jGZfR7qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "uMbAkDEaR_NT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b135674b-ff53-454e-b196-351a773841e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 281.4 MB 43 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 199 kB 47.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=538b22f7ad771a42a95dfe029f49af71e8f289aeed71224c52440996392cc70d\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\")\\\n",
        "        .appName(\"colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .config(\"spark.driver.memory\", \"5g\")\\\n",
        "        .getOrCreate()\n",
        "# Check Spark Session Information\n",
        "spark"
      ],
      "metadata": {
        "id": "wXaCnza7SV1u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "deb11eea-fa21-4554-8bee-9ac12a69d711"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f53ca920a50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2ed75d3ae2d2:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json(\"drive/MyDrive/yelp/Dataset Yelp/user.json\")"
      ],
      "metadata": {
        "id": "LchV5zCLXzYD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "z2WRpig3YcMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c625ff3-537e-46fd-85aa-8a7fee3ef9b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- average_stars: double (nullable = true)\n",
            " |-- compliment_cool: long (nullable = true)\n",
            " |-- compliment_cute: long (nullable = true)\n",
            " |-- compliment_funny: long (nullable = true)\n",
            " |-- compliment_hot: long (nullable = true)\n",
            " |-- compliment_list: long (nullable = true)\n",
            " |-- compliment_more: long (nullable = true)\n",
            " |-- compliment_note: long (nullable = true)\n",
            " |-- compliment_photos: long (nullable = true)\n",
            " |-- compliment_plain: long (nullable = true)\n",
            " |-- compliment_profile: long (nullable = true)\n",
            " |-- compliment_writer: long (nullable = true)\n",
            " |-- cool: long (nullable = true)\n",
            " |-- elite: string (nullable = true)\n",
            " |-- fans: long (nullable = true)\n",
            " |-- friends: string (nullable = true)\n",
            " |-- funny: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- review_count: long (nullable = true)\n",
            " |-- useful: long (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- yelping_since: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "id": "rMl21r-XYvgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPxv-TzYZBnE",
        "outputId": "2a865aee-b818-4d62-ff4e-bf9060588b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1987897"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pd = df.toPandas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0_JHCsbTZ6Cs",
        "outputId": "252813af-518a-4899-b3e2-29a3a15f11bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a60cc9ca03fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \"\"\"\n\u001b[1;32m    816\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o36.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 10 tasks (1091.0 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    }
  ]
}